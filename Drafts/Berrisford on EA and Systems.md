---
tags:
  - Checkland
---
"Common definitions of a system do not enable us to distinguish things that are systems from things that are not" (Salado and Kulharni, 31st Annual INCOSE International Symposium, July 2021).

The term "system" is widely employed in a loose and largely useless sense (the legal system, education system, transport system, etc). If you call yourself a "system thinker", you should know something of what the great systems thinkers have tried to tell us about what a system is – as they see it.

This is a series of three related articles in one.

Article 1: On general system theory

Article 2: On cybernetics and EA

Article 3: On "soft systems" and EA

## Article 1: On general system theory

Ideas that prefigured modern “system science” emerged centuries ago. For example:

- Isaac Newton (1642-1726) described the universe as a system of objects interacting through forces, governed by the laws of motion.
- Adam Smith (1723-1790) described a nation's economy as a system of interacting businesses (autonomous agents), governed by the law of supply and demand.
- Charles Darwin (1809-1882) wrote on the evolution of a species by the reproduction of individuals with modification.
- Claude Bernard (1813-1878) wrote on the maintenance of a system by homeostatic feedback loops. Other biologists in the 19th century described other organic systems.

A more general view of systems emerged after World War Two, in the movements called “general system theory” (this first article) and “cybernetics” (see the second article below).

The record maintained by Bertalanffy Center for the Study of System Science reveals the idea of a society for the development of general system theory was conceived at the 1954 meeting of the American Association for the Advancement of Science. Following the meeting, Bertalanffy, Rapoport and Boulding founded the “International Society for the Systems Sciences”, and later “The Society for General Systems Research”.

### Bertalanffy

Ludwig von Bertalanffy (1901-1972), was a biologist who promoted the cross-science notion of a system. He wrote:

- “There exist models, principles, and laws that apply to generalized systems or their subclasses, irrespective of their particular kind.”

For example, general system concepts include encapsulation,  input and output, part and relationship, state and process, hierarchy and anarchy. Also, the idea that system elements interact to do things they cannot do alone.

**Smaller is different**

Bertalanffy's writings were influenced by his knowledge of biology. His discussion of hierarchical composition and decomposition as a general concept reflects the way biologists describe a biological entity as a hierarchical structure - an animal's body contains organs, which contain cells, which contain organellles, etc.

If you successively decompose the material universe into ever smaller components of different kinds (from galaxy to solar system, all the way to atomic particle) you will several times cross the boundary from one domain of knowledge or system of interest to another. There is no hope of integrating a discussion of coarse-grained components (say, organs of the human body) with a discussion of atomic particles.

The successive decomposition of a physical entity into ever smaller parts is not well-called systems analysis. Modern system theory is much about how elements of one particular kind interact to produce effects of interest to the observer. And system _decomposition_ (as in enterprise architecture) is about decomposing elements of _one kind_ into smaller elements of the _same kind_ (as in a function or capability hierarchy).

See the discussion of **Anderson's hierarchy** later this article.

Having noted that information is central to system theory, Bertalanffy found it difficult to draw a clear dividing line between general system theory and cybernetics, where the storage and use of information is also central.

### Rapoport

Anatol Rapoport (1911-2007) was a mathematical psychologist and biomathematician who made many contributions to system theory. In researching cybernetic theory, he pioneered the modeling of parasitism and symbiosis. This gave a conceptual basis for his work on conflict and cooperation in social groups.

In the 1980s, Rapoport won a computer tournament designed to further understanding of how cooperation could emerge through evolution.

When actors interact, they may cooperate as in a football team, or compete in a game of chess or a market place, or hurt each other, as in a boxing match or a war. Rapoport developed _game theory,_ which is about cooperation, conflict and conflict resolution. He was recognized for his contribution to world peace through nuclear conflict restraint. Watch [this video](https://www.youtube.com/watch?v=mScpHTIi-kM) on how game theory explains the evolution of cooperation.

Rapoport also developed _social network analysis._ He showed how to measure flows through large networks, study material and information distribution through a system or society, and what speeds or impedes these flows. Modern network analysis is discussed in another article.

### Boulding

Kenneth Boulding (1910-1993) might have been the first to discuss the application of general system theory to “management science”. His 1956 article is probably best remembered for drafting a hierarchy of systems, ranging from clockwork mechanisms to social systems. Boulding pointed out that

- “The [hierarchy] might serve as a mild word of warning even to Management Science. This new [system theory] represents an important breakaway from overly simple mechanical models in the theory of organization and control… Nevertheless… these advances do not carry us much beyond the 3rd and 4th levels, and in dealing with human personalities and organizations we are dealing with systems in the empirical world far beyond our ability to formulate.”

In other words, a real-world social entity cannot be modeled as a system. We can only model particular aspects of its behavior as systems. Boulding also pointed out that

- "The unit of [social] systems is not perhaps the person but the "role" - that part of the person which is concerned with the organization or situation in question. Social organizations might be defined as a set of roles tied together with channels of communication."

Today, business architect do indeed define the behavior of actors in a real organization in terms of roles they play, and the information they exchange in the course of completing regular business processes.

In 1968, Bertalanffy wrote Boulding’s hierarchy was “impressionistic and intuitive with no claim for logical rigor”. Still, it is common to arrangeme sciences in a hierarchy.

### Anderson's hierarchy of sciences

Philip Anderson (possibly unaware of Boulding and Bertalanffy), wrote a famous paper called “More is Different” on the emergence of new laws as we move up the hierarchical structure of sciences.

- “One may array the sciences roughly linearly in a hierarchy, according to the idea that the elementary entities of science X obey the laws of science Y…
- At each stage [in the hierarchy of sciences] entirely new laws, concepts, and generalizations are necessary, requiring inspiration and creativity to just as great a degree as in the previous one. Psychology is not applied biology, nor is biology applied chemistry.” Philip Anderson

In other words.

- The elements of an entity in a **higher level science** obey the laws of a lower level science.
- The people in an organization studied in **sociology** obey the laws of psychology.
- The organs of a person studied in **psychology** obey the laws of physiology.
- The cells of an organ studied in **physiology** obey the laws of cell biology.
- The organelles in a cell's **biology** obey the laws of molecular biology.
- The molecules in **molecular biology** obey the laws of chemistry.
- The atoms in **chemistry** obey the laws of solid state or many-body physics.
- The particles in **solid state or many-body physics** obey the laws of particle physics.
- The particles in **particle physics** obey the laws of quantum mechanics.

Anderson’s paper on the hierarchy of sciences is erudite and informative. However, some draw questionable conclusions from it.

**Is science reductionistic?** Does it reduce everything to physics, or to atomic particles? No. Each higher level science has its own domain-specific vocabulary for the elements of interest and their qualities, from the speed and direction of neutrinos, and the weights of planets, to strength of a friendship and the beauty of a face.

**Are higher level systems more complex than lower level ones?** No. The complexity of a solar system has nothing to do with the complexity of life on a planet within it. Complexity is a measure of how elements are related in a system of interest, regardless of complexity inside those elements, which is studied in a different science.

**Does mathematics sit at the bottom of the hierarchy of sciences**? No. Each science is interested in different qualities. We must first understand a quality if we want to quantify its amount or strength. At every level, scientists measure the strengths of qualities that interest them, and how the strength of one quality affects the strength of another quality.

**Does general system theory integrate the sciences?** No. Remember from the "knowledge structures" article that a generalization of specializations does not integrate them. A general system theory does not integrate the laws of psychology with the laws of quantum mechanics; they remain distinct domains of knowkedge. Knowing one does not help you know the other.

**Do the elements of a system in a higher science obey only the laws of a lower science?** Contrary to Anderson's conjecture, planets are elements in astrophysics, yet one planet is studied in geology. Riders are elements in the physics of a bicycle in motion; yet a rider may be a studied in biology and psychology.

### Emergence

For a modern take on the emergence of higher level systems from lower level systems and atomic particles, try this video by Sabine Hossenfelder.

[https://www.youtube.com/watch?v=Tw9sr05Vtso](https://www.youtube.com/watch?v=Tw9sr05Vtso)

### Disenchantment

In the 1950s, the idea of a general system theory stimulated social scientists to look afresh at human organizations. Increasingly, the term "systems thinking" became associated with the organization, motivation, and management of people to meet some declared aims.

Bertalanffy, Rapoport and Boulding grew so disenchanted with the diversion of systems into "management science" or "organization theory" that they tried to restrain this shift. David Pouvreau wrote that

- “Bertalanffy, Rapoport and Boulding … had to take into account the trends that asserted themselves… Rapoport’s resignation in 1977 corroborates this interpretation. He took his decision with regard to the predominance [in their Society of] “managerial” applications… Rapoport and Bertalanffy tried to restrain this evolution… and Boulding also actively joined them.”

Also:

- “Two main directions can be observed… The first … connected to the history of early cybernetics.… gradually reduced on behalf of another … oriented toward… the multitude of organizational issues arising in contemporary society.” David Pouvreau

Unfortunately, the ambiguity of systems thinking terms and definitions has allowed the schism to continue, largely unacknowledged, And the reluctance of some systems thinkers to acknowledge the schism has led to considerable confusion.

### Remarks

It is interesting to compare and contrast how generic concepts (such as input and output, part and relationship, state and process, hierarchy and anarchy) surface in different sciences.

However, it would be difficult to claim general system theory has (as was hoped) unified the sciences. The sciences retain their domain-specific languages and laws. And we have to address differences between systems in which the elements are

- a) activities that increase/decrease a set of quantitative variables
- b) actors that interact in activities to meet aims.

And distinguish between a _social entity_, a community of actors that evolves both naturally and by design, and a _social system_ that cannot change or replace itself, because it is a particular way of behaving, expressible in an abstract system.

### Further reading

It is not enough to say general system concepts include encapsulation, input and output, part and relationship, state and process, hierarchy and anarchy, and the idea that system elements interact to do things they cannot do alone. A useful system theory has to relate some or all of those concepts in a coherent system of system concepts.

In the 1950s, Ashby (article 2 below) pointed out that one physical entity can be described from different view points as different systems. In the 1970s, Checkland (article 3 below) pointed out that one human enterprise can be described from different view points as different systems.

Today, people still confuse the two concepts. A physical entity is a material body. An enterprise is a social entity directed by directors to meet their aims. A system is an abstraction, a description that qualfies as a system because it conforms to an accepted system theory.

The two articles below relate schools of systems thinking to enterprise architecture (EA). For a more sociological perspective of systems, See the related articles on Ackoff and Meadows.

## Article 2: On cybernetics and EA

If you call yourself a "system thinker", you should know something of what the great systems thinkers have tried to tell us about what a system is – as they see it. This article is on cybernetics. Unless otherwise stated, the quotes below are from Ashby’s [Introduction to Cybernetics](http://pespmc1.vub.ac.be/books/IntroCyb.pdf).

The first cyberneticians included:

- Norbert Wiener (1894-1964)
- W. Ross Ashby (1903-1972)
- Alan Turing (1912-1954).

When Norbert Weiner coined the term "cybernetics”,  he was paying homage to James Maxwell's 1868 paper on “governors”. The word derives via Latin from the Greek Kubernetes, meaning "steersman".

Cybernetics explains how information can be communicated and stored to monitor and steer the state and behavior of animals and machines. It is much about a system in which

- One (regulator or controller) subsystem monitors and directs
- Another (regulated or target) subsystem.

In the mid 20th century, cybernetics was discussed and promoted by two influential groups.

- 1941-1960: The Macy Conferences were cross-disciplinary meetings in New York, with a mandate to aid medical research. Topics included connective tissues, metabolism, the blood, the liver, renal function, infancy, childhood, aging, nerve impulses, and consciousness.
- 1949-1958: The Ratio club was a cross-disciplinary group in the UK, founded by the neurologist John Bates to discuss cybernetics. Members included psychologists, neurobiologists, engineers, physicists, and mathematicians. Many went on to become prominent scientists.

W. Ross Ashby was a psychologist and a prominent member of the Ratio Club, who popularized using the term “cybernetics “with reference to self-regulating systems. His work influenced many scientists, and is the basis for this article.

### Ashby’s cybernetics

Ashby's cybernetics defines a system, in the abstract, as a set of state variables and a way of behaving. He told us:

- a dynamic system is a way of behaving, not a material entity.
- we are concerned with behaviors that are regular and reproducible.
- it is the behavior we study, not the material substance.
- a dynamic system changes with time.
- change occurs by a measurable jump from one state to another, or one way of behaving to another.

Let me define a system in similar but slightly more generic way.

**System** (1): one or more behavior patterns, performable (in the abstract), or performed (in reality), by elements that interact to advance the state of a thing.

Ashby emphasised that the characteristics of a system are defined by an observer with some given interest in mind. The role of an observer can involve

- deciding what is relevant to some interest or concern
- describing and relating system structures and behaviors
- observing real system inputs and outputs, and
- playing a role as a user of, or actor in, the system of interest.

### Abstracting systems from physical entities

Checkland said a system is a perspective - a way of looking at some entity or situation we observe or envisage. Ashby said the viewpoint we take is always with respect to some "given interest" chosen for attention.

- “It is important to stress Ashby defined a system not as something that exists in nature. A system consisted of a set of variables chosen for attention and relationships between these variables, established by observation, experimentation, or design." Ashby’s student Krippendorff writing in 2009 on [Ashby’s Information Theory](https://www.researchgate.net/publication/49128560_Ross_Ashby%27s_Information_Theory_A_Bit_of_History_Some_Solutions_to_Problems_and_What_We_Face_Today)

Ashby's system is independent of any physical form or medium for a memory or a message. It is abstracted from the infinite complexity of any physical entity or situation (or real machine, as he called it) in which that system may be observed.

Consider Ashby's model of the stickleback mating ritual (after Tindbergen).

![](https://media.licdn.com/dms/image/v2/D4E12AQGsi_3kDobSXg/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1704286192681?e=1729728000&v=beta&t=rZFcgXEXvii3nUiob_1-rZX0Vcg_x6gP0VAhvdGX5GM)

Stickleback mating ritual as a progressive dialog

The _abstract system_ is the roles and rules in the ritual typified above. A r_eal system_ is one instance of the ritual, as performed by a _socio-material entity_ including two sticklebacks, a nest and some eggs.

- "[We] relate the behaviours of a real physical system to the properties of symbolic expressions, written on paper. The [real system] is said to embody the [abstract system]." (Introduction to Cybernetics.)

Related articles elaborate on the distinction between abstract systems (types) and real systems (instantiatons of abstract systems).

### The state of a system

Ashby scoped a system as a set of inter-related state variables, which may be selected from observation of some real-world entity or phenomenon.

A system’s state can include qualitative variables (like the color, position and motion pattern of a stickleback), but Ashby's work was grounded in mathematics, and he was keen to demonstrate his principles mathematically. So he focused on quantitative variables, such as the angle of a pendulum, the amount of a resource, or a voltage whose quantitative value goes up and down over time.

Ashby wrote the following words; I have re-sequenced them.

- "We are concerned with those aspects of systems that are determinate - follow regular and reproducible courses. Every dynamic system has many distinguishable states. [Being] a determinate machine, fixing its circumstances and the state it is in will determine the state it next moves to." (Introduction to Cybernetics.)

At this point, Ashby was concerned with a closed system that advances from state to state under its own internal drive. Later, he introduced the idea of a system that receive inputs, which influence the next state of the system.

In “Design for a Brain”, Ashby defined the state of a system thus:

- 2/9. The _state_ of a system at a given instant is the set of numerical values which its variables have at that instant.
- 19/2. Each _variable_ is a function of time.

**Some terminology**

Throughout this article I include definitions of terms included also in a systems thinking vocabulary of nearly 200 terms.

**Variable:** a data structure representing a quantity, quality or status of an entity. Quantitative variables include amounts such as populations and weights. Qualitative variables include types and attributes such as colours and addresses. State variables can symbolize or characterize the stages in a life history, such as egg, maggot and fly.

**State:** the current values of a system’s state variables. For example, consider

- the state of an ecology, represented by predator and prey populations
- the state of a tennis match, represented by game, set and match scores
- the state the entities and events that a business monitors and directs, represented in one or more databases.

The state of a system changes, under its own internal drive and/or in response to inputs.

**Microstate:** the state of atomic parts, their values or quantities (for instance, the mass of each organism).

**Macrostate:** the state of aggregate system-level properties that emerge from system behavior (for instance, total biomass).

The idea of micro and macro states implies only two levels. There is one whole composed of several atomic parts. Think of molecules in a gas cloud, or people in a society.

Often, when we describe a large machine, organism or business, we define actors and activities and aims at several levels of granularity, by hierarchical composition and decomposition.

**Hysteresis:** the idea that a system’s current state derives from its history. The state of the system today depends on the state that past events have left it in. (In software systems, the current state can be generated by replaying past events.)

### The behavior of a system

Ashby's system is way of behaving, rather than a material thing. He defined a system by how it behaves over time, as a set of regular or repeatable state changes. This shift in perspective, from the physical structure of a system to its logical behavior, is central to the cybernetic view of the world.

- 2/10. A _line of behaviour_ is specified by a succession of states and the time-intervals between them. The first state is the initial state." ([Introduction to Cybernetics](http://pespmc1.vub.ac.be/books/IntroCyb.pdf)).

**Line of behavior:** the life history of a system's state as its state variable values go up and down, be it regularly or chaotically. This is an inevitable result of the system following its rules.

Given a system with two or three state variables, its line of behavior may be represented as a two or three-dimensional geometric shape. Consider a pendulum for example.

![](https://media.licdn.com/dms/image/v2/D4E12AQEQjSjPlYNoeg/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1704203722977?e=1729728000&v=beta&t=0NJDpqZqJ3y0XTHOlSzz3d9ow80MO7A1k0skqTeUs7k)

Pendulum state changes (copyright Wikipedia)

Another article discusses some related concepts including attractors (such as the vertical state of a pendulum), and basins of attraction.

### Open systems

All activity systems, open and closed, respond to internal events or state changes, and produce internal events or state changes.

Ashby wrote first of closed systems - of machines and organisms that change from state to state under their own internal drive, without any inputs.

**Closed system:** a system that behaves independently of its environment or wider world.

Some say a clock is a closed system, but it consumes input energy and displays the time to observers. Some say a System Dynamics model is a closed system, but when it runs in a computer, it is given initial state variable values, and reports the state changes it experiences over time. Some say the universe is the only truly closed system.

Ashby wrote later also open systems that interact with their environment.

**Open system:** a system connected to external entities by consuming inputs or producing outputs. You might say it transforms inputs into outputs/

To define an open system by its inputs and outputs is to define its boundary - to separate what is inside and outside the system -– to encapsulate the activities it performs – to define the interface between the system and its environment.

- 3/8. "Given an organism, its _environment_ is defined as those variables whose changes affect the organism, and those variables which are changed by the organism's behaviour." (Design for a Brain)
- 3/11. "The organism affects the environment, and the environment affects the organism, such a system is said to have feedback." (Design for a Brain).

### Ashby’s good regulator theorem

- "Every good regulator of a system must be a model of that system" Conant and Ashby

A little more accurately, a regulator must contain, or have access to, a model of the system it regulates. A regulator must know the true state of whatever system regulates - be it animal or machine. The idea that regulators <create and use> models to <represent the state of> targets they <monitor and direct>, can be represented in a triadic graphic.

![](https://media.licdn.com/dms/image/D4E12AQFlL_qqiFndpw/article-inline_image-shrink_1500_2232/0/1722421621423?e=1729728000&v=beta&t=Oc-x3s7f9MELtO_YN2W-qtgRKr0Ux0MlLl93C1eYmB0)

The good regulator theorem

Ashby and Conant put it this way

- "The design of a complex regulator often includes the making of a model of the system to be regulated. The making of such a model has hitherto been regarded as optional, as merely one of many possible ways. In this paper a theorem is presented which shows, under very broad conditions, that any regulator that is maximally both successful and simple must be isomorphic with the system being regulated. Making a model is thus necessary.” Roger C. Conant and W. Ross Ashby

In general, any regulator that monitors or directs a target entity must have a model of that target entity's state. More particularly, any animal, machine or business that seeks to monitor or direct some other entity or event, must have (or have access to) a model of it.

Ashby said that a regulator’s model of a target must be _isomorphic_ with the target that is being regulated. In other words, a regulator’s model of a target can be _correlated with_ elements and relationships in reality.

A controller/regulator typically contains three kinds of component:

- Receptors that sense changes in the state of a target.
- Control center that determines the regulator’s responses to those changes.
- Effectors that act to change the state of a target.

![](https://media.licdn.com/dms/image/v2/D4E12AQHLgd9bAIe1OA/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1722255816322?e=1729728000&v=beta&t=yBITpSU2SL5p43dGQ1CMaQst9hVNkCZxJHoWpnbhsA0)

Regulation of a target system

If a regulator is to monitor and direct the state of a target, then it must know or remember the state of that target, and update that knowledge. A regulator does this by means of a feedback loop that connects the regulator and target subsystems such that the output from one is input to the other.

### Ashby’s “design for a brain”

In “Design for a Brain” Ashby presented the brain as a regulator that monitors and maintains the state of the body. Brain cells interact to maintain body state variables by receiving messages from sensors, processing them, and sending message to muscles and glands.

![](https://media.licdn.com/dms/image/D4E12AQGRXByZo5j8zw/article-inline_image-shrink_1500_2232/0/1722255868334?e=1729728000&v=beta&t=RxK-mMrWP4_0LsIFqpxI1PJXHhW5b5xEZ0jvaimEjT8)

Ashby's Design for a brain

For example: your sensors detect a change in your body’s temperature; the brain determines the response to that change and directs various glands and muscles, which act to increase or reduce the body temperature.

Feedback loops of these kinds are found animal, mechanical and software control systems. A missile guidance system senses spatial information, and then sends messages to direct the missile.

More advanced brains create and use mental models of the external world - enabling the animal to recognise and respond to changes in that world.

Ashby’s idea may be generalised from control systems to other kinds of information system that capture information from things in their environment, and respond to or direct those things.

![](https://media.licdn.com/dms/image/D4E12AQFN1X-5K9s1MA/article-inline_image-shrink_1500_2232/0/1722255927115?e=1729728000&v=beta&t=Wc-dz8aBqfXNr3BtsTPbN-EM0NtmKyq4ZTq9WJ1pTu4)

Open activity system

**Activity system:** Actors interact in orderly activities to consume inputs from external entities (in the wider environment), maintain system state, and deliver outputs to external entities.

### Ashby’s law of requisite variety

**The law of requisite variety:** the model used by a control/regulator system must have the variety it needs to detect changes that matter in its target/regulated system or environment.

Obviously, an animal must monitor and manipulate things in its environment. Since the environment has infinite variety, it must _limit_ the events it detects and the states it can distinguish, to what matters for its survival. On the other hand, it needs the _enough_ information to recognize significant changes in its environment.

Central to cybernetics is the idea that regulator or control system collapses the infinitely complex phase space of its target to a simple model, including the variables it needs, but (for optimal efficiency) no more than it needs.

![](https://media.licdn.com/dms/image/v2/D4E12AQHMiXAGmFtnlQ/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1722255974432?e=1729728000&v=beta&t=1L9LSm6Ykd6xkGeCF83C7YDP1Vx_9IBwT3FnSJN8dgk)

A regular must have the requisite variety to direct a target

Ashby’s law does not imply a controller recognizes every state of its target, only those states that matter to advancing that controller’s goals. And note that one entity might be regulated by several controllers, with the same or conflicting goals.

### Applying the law in practice

Ashby hoped his cybernetic principles for the regulation of a target by a controller could be applied to large and complex entities like a business or a human society. However, Ashby's law can be misinterpreted, and practical challenges to applying it include the following.

**Identifying the relevant state variables**

The designer of a control system must identify the critical state variables of the entity to be controlled. This is reasonably straightforward when designing a system that is required to consume or produce particular messages or reports. Where the thing to be regulated is a large and complex social and/or economic entity, in which human actors can act as they choose, that can be challenging.

**The many-to-many relationship between systems**

Ashby wrote of cases where one control system regulates one target system. The coupling of systems in reality is often far more complex. One controller may regulate several entities; and several controllers may cooperate, or else compete, to regulate one entity.

See other articles for more on these two challenges.

### Ashby on encoding and decoding

Again, Bertalanffy noted that information is central to system theory. Boulding said the same.

- “singled out for special treatment is the theory of information and communication…we must concern ourselves with the content and meaning of messages … The empirical universe here is human life and society in all its complexity and richness.” Kenneth Boulding (1956)

Ashby wrote on the ubiquity of encoding and decoding in the communication of information. He exposed the many data structures in involved a “Gale warning” broadcast.

_“It starts as some patterned process in the nerve cells of the meteorologist, and then becomes_

- _a pattern of muscle-movements as he writes or types it, thereby making it_
- _a pattern of ink marks on paper. From here it becomes_
- _a pattern of light and dark on the announcer’s retina, then_
- _a pattern of retinal excitation, then_
- _a pattern of nerve impulses in the optic nerve, and so on through his nervous system._

_It emerges, while he is reading the warning, as_

- _a pattern of lip and tongue movements, and then travels as_
- _a pattern of waves in the air. Reaching the microphone it becomes_
- _a pattern of variations of electrical potential, and then goes through further changes as it is amplified, modulated, and broadcast. Now it is_
- _a pattern of waves in the ether, and next_
- _a pattern in the receiving set. Back again to_
- _the pattern of waves in the air, then_
- _a pattern of vibrations in listener’s ear-drums, then becomes_
- _a pattern of nerve-impulses moving up the auditory nerve.”_

Marvelously, through no less than sixteen major transformations, from one data structure to another, the sender’s meaning is preserved and understood by the listener.

We encode descriptions of what we observe and envisage in memories (for recall) and in messages (for sharing). We share information by encoding and decoding information (meaning) into and out of data structures such as sounds (oral language) and visual patterns (writing). We also use writing as an external form of memory.

### Systems and processes

A system may cycle around from one state, through others, and back to the same state. If a system never settles in a steady state, or repeats any behavior, we often call it a process (a progression of unique states) rather than a system.

A process, like the stickleback mating ritual, may be what Ashby called a “transient” system in the life history of a longer-lived entity in which the process is repeated.

In the long-term, any real world entity in which we observe a system will eventually stop, due to the emptying of a resource, the decay of parts or a major disruption of some kind.

And by the way, beneath a self-driven steady state system of aggregates (like the number of sticklebacks and insects in a pond), there is often an event-driven system in which individual entities are born, live and die.

### Ashby’s hope and challenges

Ashby hoped the principles he developed, for defining the behavior of small systems (in the form of state variables, governed by rules that determine how those variables’ values change over time) could be applied to large and complex entities.

- “cybernetics is likely to reveal a great number of interesting and suggestive parallelisms between machine and brain and society. And it can provide the common language by which discoveries in one branch can readily be made use of in the others.” (Introduction to Cybernetics, section 1/6.)

If cybernetics is to scale up, Ashby recognised it must address two particular issues, "system change" and "self-organisation" which are both ambiguous terms.

Ashby explained "self-organization" by coupling a system of interest (one to be re-organized) to what I call a "higher" entity or process (an organizer). This is sometimes called "double loop adaptation”.

See the article "[Seven kind of system](https://www.linkedin.com/pulse/systems-rules-self-organization-graham-berrisford-28fge)" for an explanation of self-organization by double loop adaptation.

### The influence of cybernetics on others

Cybernetic concepts include system-environment boundary, inputs, outputs, processes, roles, rules, state, information, hierarchy and self-organization. Cybernetic principles include Ashby's good regulator theorem, and his law of requisite variety.

Today, these concepts and principles are used unknowingly by enterprise business and data architects, when they model business activity systems with a view to changing and replacing them.

Ashby's cybernetics also influenced many scientist and well-known systems thinkers including one discussed in later articles (Stafford Beer, Jay Forrester, Russell Ackoff, Donella Meadows, and Peter Checkland**)** and the three luminaries below.

**Ludwig von Bertalanffy**

Cybernetics influenced the father of "general system theory". The two schools overlap so far that, I am told, Bertalanffy found it difficult to delineate them.

**Peter Senge**

Senge employed cybernetic concepts when he represented his social _system archetypes,_ such as "the tragedy of the commons", in causal loop diagrams.

**Heinz von Foerster**

In the 1970s, von Foerster used the term “second order cybernetics” in discussing how observers relate to systems.

- “First-order cybernetics is the science of observed systems; Second-order cybernetics is the science of observing systems... What we need now is the description of the "describer" or, in other words, we need a theory of the observer”. Heinz von Foerster

Looking back, much of what people say under the banner of second order cybernetics i was already embraced by Ashby in classical cybernetics.

- Ashby emphasised that system characteristics (variables and rules) are defined by an observer with some given interest in mind.
- Ashby said observers not only observe and define abstract systems, but are coupled to real systems when providing inputs and consuming outputs.
- Ashby regarded “self-organization” as a double-loop adaptation pattern.

Much of what is discussed under the heading of second order cybernetics might better be called organisation theory or social entity thinking than activity systems thinking.

Will Harwood tells me he concluded von Foerster was striving to highlight the distortion that emerges when an observer, having predicted the outcome of a system, or set a target for it, then acts to play a role in that system. As when a major investor predicts a change in the stock market, then buys shares.

### Mapping EA to cybernetics

Here, EA stands for enterprise architecture, which embraces business architecture, and the architecture of information systems that support and enable business activities

**22 Points of correspondence**

The numbers included in some points below are chapter/section numbers in Ashby’s [Introduction to Cybernetics](http://dspace.utalca.cl/bitstream/1950/6344/2/IntroCyb.pdf) (1956)

1. We sit our system in its environment.
2. We study the dynamics of how the system reacts to its environment, and the environment reacts to it.
3. We select the set of state variables the system maintains, its state vector.
4. Ashby’s good regulator principle (defined above) applies. A business activity system maintain state variables that record attributes of entities and events that the system monitor and directs.
5. Ashby’s law of requisite variety (defined above) applies.
6. Wrt 1/2 we ask not “what is this thing?” but “what does it do?” Our approach is essentially functional and behaviouristic. A dynamic system is a way of behaving not a material entity; it is the behavior we study not the material substance.
7. Wrt 1/3 we are concerned with information rather than energy.
8. Wrt 1/4 our abstract system model is of sets rather than particular members, types rather than particular instances.
9. Wrt 2/3 we can define each state change (be it event-driven or autonomic) as a transformation from one state to the next.
10. Wrt 3/1 we are concerned with behaviors that are regular and reproducible. Even though our systems may be “non- linear, non-continuous, and not even metrical, i.e. expressible in number.”
11. Wrt 3/5 the macro state  vector of a system is an aggregate of its parts’ micro states.
12. Wrt 3/11 we cannot study the whole of a substantial material, biological or social entity. We must pick out and study the variables relevant to the aims we have. (Ashby never pretended holism means wholism.)
13. Wrt 4/1 our systems change with time. A change occurs by a measurable jump from one state to the next, or one "way of behaving" to another.
14. Wrt 4/4 we attend to the inputs and outputs of a system.
15. Wrt 4/6 we attend to the coupling of systems to each other and to external entities. The coupled systems must remain distinct.
16. Wrt 6 we encapsulate a system behind its I/O interface. Note that to define what it does (in the pre and post conditions of events) we must know its internal state variables (regardless of where and how they are stored).
17. Wrt 7/2 learning (and by implication, modeling a system) is only worthwhile in a stable environment.
18. Wrt 8/2 the transmission of information, the encoding and decoding of information into and out of data structures, is of central importance.
19. Wrt 8/5 our systems exchange messages with each other and external entities.
20. A system is deterministic, meaning the next state is determined by its present state and the values of variables in input messages.
21. Wrt 13/2 a difficulty in modeling large systems is the variety of events the system must respond to.
22. Wrt 13/3 another difficulty is the hierarchical division of a system into subsystems, then into sub-subsystems and so on. At every, level the lower-level subsystems are more closely coupled than the higher level ones. So, as a general rule, the finest-grained systems are the most tightly-coupled; the coarsest-grained are the most loosely-coupled. (A point to be born in mind by those who refer to Larry Constantine's principles of coupling and cohesion, set out more than a decade later.)

Point 6 would be recognised by "soft systems" thinkers like Churchman and Checkland. Checkland’s concern was not the material substance of a business's resources but the “transformations” it makes and the activities it performs to produce desired effects.

Point 17 is true of EA. If the environment is changing rapidly in an unpredictable way, then investing in the design of a large and complex system can be hostage to fortune. One may do better to apply the agile principles of KISS and YAGNI to changes that are designed, planned and made.

### 4 Points of difference

Cybernetics and EA share many concepts and principles, notably the good regulator theorem, the law of requisite variety, encapsulation behind I/O interfaces, and hierarchical system decomposition. We can see them as branches of one general system theory. However, as a reader has commented:

- “It’s not surprising there are similarities, given that Bertalanffy (GST) and Ashby (Cybernetics) took physics as their starting point. This immediately gives us the ideas of variables, dynamics and state transitions. The differences are important if you wish to do anything that might be called calculational or predictive.

Later articles will point to issues that hinder predictability, even in cybernetic systems. Having said, that four differences should be highlighted.

**Information**

Cybernetics and EA are both concerned with the encoding and decoding of information, but treat information differently.

**"Way of behaving"**

Cybernetics and EA are both concerned with behaviors that are regular (rule-bound) and reproducible. Ashby spoke of a system being defined by its "way of behaving", meaning its “line of behavior”, rather than why state changes happen, or their mechanics.

By contrast EA is much concerned with requirements, rules and roles - with the requirements for a system, the processes performed, and the roles of actors in those processes. I shall say a lot more about the rules that govern state changes when event/input messages arrive.

**Homeostasis**

Cybernetics and EA are both concerned with state changes. Ashby was especially interested in systems that are stable in the homeostatic sense they maintain the state variables of an entity in a desired range, which helps the entity survive.

Business activity system are better characterised as progressive. In response to inputs, the state of the system advances in step with the state of external entities and events.

**Paradigm**

Ashby discussed self-driven machines and continuous dynamics. Enterprise or business activity systems are event driven automata.

A biological entity or software application recognizes a wide variety of input types, in any order, and must respond to each event by applying a different subset of its rules to a subset of its state variables. Might Ashby say each input type triggers a different system? Although it has several ways of behaving, most see such an assembly of input types, rules and variables as one system.

### Further reading

See the articles on cybernetics and System Dynamics for more thoroughly worked out general system theories. See the article "[Seven kind of system](https://www.linkedin.com/pulse/systems-rules-self-organization-graham-berrisford-28fge)" for an explanation of self-organization by double loop adaptation. See the articles on Ackoff and Meadows for a sociological perspective of systems.

## Article 3: On "soft systems" and EA

If you call yourself a "system thinker", you should know something of what the great systems thinkers have tried to tell us about what a system is – as they see it. This article relates systems of the kind discussed in enterprise architecture (EA), soft systems and cybernetics. It includes  20 points of correspondence between the EA and  cybernetics, and 4 points of difference.

### Two kinds of system

In cybernetics, a system may be defined in terms of its state and the behavior that advances it.

- **System** (1): one or more bounded patterns of behavior performable (in the abstract), or performed (in reality), by elements that interact to advance the state of a thing.

In soft systems approaches, a system may be defined in terms of input-to-output transformations made by event-driven activities.

- **System** (2): a bounded pattern of activities performable (in the abstract), or performed (in reality), by actors that transform the external world from one state to the next, by means of inputs and outputs. See "[Conceptualising the enterprise as a system](https://www.linkedin.com/pulse/universal-ea-concept-graph-graham-berrisford-a57rc)".

A system can be viewed from either perspective, or both.

### A business as an activity system

A basic concept of Enterprise architecture (EA) is that an enterprise can be described, or modelled, as a system of systems that exchange information. To quote from TOGAF version 10

- "The TOGAF Standard considers the enterprise as a system"
- "Architecture: The fundamental concepts or properties of a system"

EA has flowered in government agencies, financial service providers and telecommunications businesses. What do such businesses have in common? Their business is centered on creating and using information to monitor and direct what citizens or customers do. Yet few enterprise architects know much or anything about cybernetics. They are more likely to know something of soft systems thinking.

### Soft systems

A system in the "soft systems thinking" of Churchman and Checkland (just as in Ashby's cybernetics) is an abstraction from a more complex reality.

In Checkland's Soft Systems Methodology (SSM), a system is a conceptual model, or business activity model, defined from the perspective (Weltenshaung) of one or more stakeholders.

![](https://media.licdn.com/dms/image/v2/D4E12AQFirN3S_EaW_A/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1722256035757?e=1729728000&v=beta&t=Np3xZp61FGdOxNqhQR9LlCSKKJrVWk42VmNLiKumFPk)

https://blog.improv-design.com/soft-systems-methodology/ssm-the-method/

Few SSM users realize Checkland adopted the older cybernetics idea that a system is an abstraction, defined by an observer with some particular interests in mind.

- “The use of the word “system” is no longer applied to the world, it is instead applied to the process of dealing with the world.” Checkland

There is no end to the use of the term "system" in a loose and useless sense. And in systems thinking discussion, people not only confuse conceptual models that are _abstract systems_ with their instantiation in the world as _real systems._ They further confuse real systems with the _physical entities_ that instantiate them.

As Checkland observed:

- “Experience shows that this [soft/hard] distinction is a slippery concept which many people find very hard to grasp. Probably because embedded in our habits is the way we use the word system." Checkland

Generally, the relationship between abstract systems and physical entities is many-to-many. This is to say,

- the (abstract) stickleback mating ritual can be realized by many stickleback pairs
- one pair of sticklebacks may play roles many (absractly defined) systems.

In other words, there is much more to know about sticklebacks than their mating ritual - and more to a physical entity than any abstract system it realizes.

Similarly, you can abstract diverse systems (marketing, finance, customer service, compliance, facilities management) from one physical business entity. These systems may have conflicting aims, and conflicting vocabularies.

Moreover, the whole business might be described (using an upper vocabulary) at a more abstract level as a higher level system that does nothing to reconcile the lower level systems.

I have wrestled with abstraction and system integration challenges in [another long article](https://www.linkedin.com/pulse/data-mesh-what-graham-berrisford-gsmkc).

### Encapsulation

It is common to represent the transformation made business activities as an open system in term of suppliers, inputs, processes, outputs and customers (SIPOC), as is represented in this diagram.

![](https://media.licdn.com/dms/image/v2/D4E12AQF0CoyAqCUrTw/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1722256058719?e=1729728000&v=beta&t=OcbRETEbF6OyUeEO8MDEJU4zrQgVWq8mobNXMDFxQCs)

A SIPOC diagram

To design a system of this kind, we begin by defining external entities affected, and the desired effects and outputs they require, before defining actors and activities inside the system.

Sometimes, the consumers of outputs and suppliers of inputs are different entities, as above. Other times, consumer and supplier are the same entity. Consider for example the user of a telephone, or a word processor.

![](https://media.licdn.com/dms/image/D4E12AQEsVZZ3wnzJ3A/article-inline_image-shrink_1500_2232/0/1722256233634?e=1729728000&v=beta&t=DLbJH8Wkbr6aptR_i3qqhWyQwbfUUtda495HgGncqpA)

Defining an open system by its user interface

### On the purpose of an open system

Soft systems methods define a system as a pattern of activities, performed by actors, _to meet some aims._ This raises many questions I don't want to dwell in this article. Briefly, the results or outcomes of an open , designed, system can be defined in terms of its effects, that is,

- internal state changes it makes
- outputs it produces (leading to external state changes)

System designers aim to produce “desired effects”, that is, state changes that sponsors or other stakeholders find beneficial. In practice, in operation, a system may produce effects judged by observers to beneficial, neutral or harmful, depending on their viewpoint.

### On granularity and modularity

The notion of hierarchical composition and decomposition is important in system science. You may see a tennis match as an atomic event, with a winner and loser. Or decompose it into sets, then games, then points (as on the score board). Then decompose further to ball strikes, and steps between ball strikes. And further, within each player, to each muscle contraction and neural impulse.

Open systems can be composed and decomposed in a hierarchy. When you widen a system's boundary, it becomes subsystem (module or component) of a wider system. Some outputs of the first system become internal state changes of the wider system, and some external entities become internal actors.

Note that if two internal structures are the same, and the components relate the same, they will produce the same behavior. By contrast, what the observer sees as one behavior (external view) can be produced by infinite internal structures.

This article does not address choices about the internal modularisation of a system. Ironically, the enterprise and software architecture courses I teach (my day job) are much about the modularity of a business, an application portfolio and an application.

### Further reading

This is one of several related articles that how show the ideas of famous systems thinkers (Ashby, Beer, Forrester, Ackoff, Meadows, Senge and others) can be embraced in a coherent system theory, provided you recognize that not everything is well-called a system, and not every approach to thinking about a social entity is well-called systems thinking.