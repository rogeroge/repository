(From: Ross Ashby’s information theory: a bit of history, some solutions to problems, and what we face today - Klaus Krippendorff)
Towards the end of his life Ross Ashby investigated the 'combinatorial explosion' of possibilities, this underpinned his thinking on complexity and was resolved by his use of constraints

From Krippendorff:

>  It was fortuitous for Ashby to met Hans-Joachim Bremermann at the second conference on self-organizing systems. Bremermann (1962) recognized that information transmission or information processing systems need to respond to differences, which cannot be arbitrarily small, thus entailing a limit, not part of information theory. In terms of Einstein's mass-energy equivalence and Heisenberg‘s Uncertainty Principle, he argued that the transmission or processing capacity of any circumscribable system, artificial or living, cannot exceed mc2/n  bits per second.  
>   
> I suggest that understanding information processes of the kind we are facing today can no longer be accomplished by discovering and identifying interactions in observed systems. In fact, reconstructability analysis quickly runs into transcomputational numbers. In a little known paper Conant (1981b) found a way to bypass Bremermann‘s limit by not selecting a solution from all possible alternatives but constructing a solution based on a simpler representation of the problem. In effect, he moved beyond the limit of observational determinability by designing a solution. Technology is not discovered, it is designed. To understand complex technological systems requires an understanding of how they are designed, how their realities are constructed within the possibilities created by their designers. Bremermann‘s limit defines the space within which human agency is physically possible.

(From: [Improvisation Blog: Ross Ashby's Constraints and Education](https://dailyimprovisation.blogspot.com/2016/05/ross-ashbys-constraints-and-education.html))

> Fundamentally, Ashby is concerned with constraint. He says of cybernetic science that cyberneticians "observe what might have happened, but did not"
>![[Media/Ashby - definition of a cyberneticist.jpg]]
> His concept of science was that minds generate ideas through building models of the world. Knowledge arises as a result of the identification of error in the models as new constraints of nature bearing upon the speculated model are discovered. This idea of constraint is fundamentally different from the traditional causal paradigm of conventional science, which sees mechanistic descriptions of event regularities. The problem with mechanistic descriptions of causes is that it is easy for the science establishment to become complacent, because it believes its function is to identify and prove mechanisms (even if philosophers like Popper rightly point out that the job is to disprove things). This complacency has led to the kind of stultification of science that we have today where practically everything revolves around data processing, and critical inquiry has been pushed out in favour of commodified knowledge and capitalist-driven science ethic.
> 
> To put the identification of constraint at the heart of a methodology is to negativise science. Most interestingly, it focuses not just on the constraints of nature in showing how a model is wrong, but it also considers the constraints which bear upon the creation of models in the first place. Cybernetic theory itself is subject to constraint - something which many cyberneticians (who wish their theories to be 'correct' because they appear sometimes in need of religion rather than science) forget. I've realised that categories I've explored before in identifying different tendencies in cybernetic theory - foundationalism, objectivism, universalism - all expose different kinds of constraint within cybernetic theory. Moreover these categories themselves are constrained - not least by each other.

This explains Ashby's focus on Systems Thinking 

"false realities are useful avoidance strategies"  
  
I think this is the nub of my concerns about complexity nonsense. Increasingly we see thinking unhinged from the physical reality in which it needs to be manifest. This is not the same as denying the role of imagination!  
  
However the costs of identifying and eliminating the clutter of this junk is becoming prohibitive. Drucker popularised the Churchill maxim "A lie gets halfway around the world before the truth has a chance to get its pants on", the equivalent here "the waste in progressing bad ideas far exceeds the benefits from any good ones".  
  
There is too much emphasis on creativity (the Voros cone) and too little on realisation (the manifold of possibility). That's the essential balance of pragmatism.

Click or press enter to display in the image preview![](https://www.linkedin.com/dms/prv/vid/v2/D4E06AQFktSEqYpNdEw/messaging-attachmentFile/messaging-attachmentFile/0/1721644978899?m=AQK-aTYI8h2plwAAAZMG2P07TJPmXrs0GvsMYVulNEcXSODVXwlO&ne=1&v=beta&t=iE1JcNY0yRoP6f1wJZ1Hg1ne4ZMgXfJY8M9sdqSy0tY)

Click or press enter to display in the image preview![](https://www.linkedin.com/dms/prv/vid/v2/D4E06AQEoqnOw5Iw3YA/messaging-attachmentFile/messaging-attachmentFile/0/1721645253068?m=AQJaIIjR3z1VZgAAAZMG2P027Mg_Pk-JBrSWohaxXDLdWMRmQQiN&ne=1&v=beta&t=65Q3Jdh5ughYPBTdAKETgqutUfBExlXCuH1ypgVxrjU)

Ross Ashby discusses the limitation of classical science which is developed from the premise that systems can be understood by 

(From: Requisite Variety and Its Implications for the Control of Complex Systems - Ashby, there is a similar argument by Stafford Beer in What has Cybernetics to do with Operational Research? )

> As I have said, we must beware of taking our strategies slavishly from physics and chemistry. They gained their triumphs chiefly against systems whose parts are homogeneous and interacting only slightly. Because their systems were so specialised, they have developed specialised strategies. 
> 
> We who face the complex system must beware of accepting their strategies as universally valid. It is instructive to notice that their strategies have already broken down in one case, which is worth a moment's attention. Until about 1925, the rule «vary only one factor at a time» was regarded as the very touchstone of the scientific method. Then R. A. Fisher, experimenting with the yields of crops from agricultural soils, realised that the system he faced was so dynamic, so alive, that any alteration of one variable would lead to changes in an uncountable number of other variables long before the crop was harvested and the experiment finished. So he proposed formally to vary whole sets of variables simultaneously-not without peril to his scientific reputation. At first his method was ridiculed, but he insisted that his method was the truly scientific and appropriate one. Today we realise that the rule «vary only one factor at a time» is appropriate only to certain special types of system, not valid universally. Thus we have already taken one step in breaking away from the classical methods. 
> 
> Another strategy that deserves scrutiny is that of collecting facts «in case they should come in useful some time»-the collecting of truth «for truth's sake». This method may be efficient in the systems of physics and chemistry, in which the truth is often invariant with time; but it may be quite inappropriate in the systems of sociology and economics, whose surrounding conditions are usually undergoing secular changes, so that the parameters to the system are undergoing changes-which is equivalent to saying that the systems are undergoing secular changes. Thus, it may be worthwhile finding the density of pure hafnium, for if the value is wanted years later it will not be changed. But of what use today, to a sociologist studying juvenile delinquency, would a survey be that was conducted, however carefully, a century ago? It might be relevant and helpful; but we could know whether it was relevant or not only after a comparison of it with the facts of today; and when we know these, there would be no need for the old knowledge. Thus the rule «collect truth for truth's sake» may be justified when the truth is unchanging; but when the system is not completely isolated from its surroundings, and is undergoing secular changes, the collection of truth is futile, for it will not keep.
> 
> There is little doubt, then, that when the system is complex, the scientist should be ware of taking, without question, the time-honored strategies that have come to him from physics and chemistry, for the systems commonly treated there are specialised, not typical of those that face him when they are complex. 
> 
> Another common aim that will have to be given up is that of attempting to «understand» the complex system; for if «understanding» a system means having available a model that is isomorphic with it, perhaps in one's head, then when the complexity of the system exceeds the finite capacity of the scientist, the scientist can no longer understand the system-not in the sense in which he understands, say, the plumbing of his house, or some of the simple models that used to be described in elementary economics.
> 
> It will now be obvious that the strategies appropriate to the complex system are those already getting well known under the title of «operational research». Scientists, guided doubtless by an intuitive sense of what is reasonable, are already breaking away from the classical methods, and are developing methods specially suitable for the complex system. Let me review briefly the chief characteristics of «operational» research. 
> 
> Its first characteristic is that its ultimate aim is not understanding but the purely practical one of control. If a system is too complex to be understood, it may nevertheless still be controllable. For to achieve this, all that the controller wants to find is some action that gives an acceptable result; he is concerned only with what happens, not with why it happens. Often, no matter how complex the system, what the controller wants is comparatively simple: has the patient recovered?-have the profits gone up or down?-has the number of strikes gone up or down? 
> 
> A second characteristic of operational research is that it does not collect more information than is necessary for the job. It does not attempt to trace the whole chain of causes and effects in all its richness, but attempts only to relate controllable causes with ultimate effects. 
> A third characteristic is that it does not assume the system to be absolutely unchanging. The research solves the problems of today, and does not assume that its solutions are valid for all time. It accepts frankly that its solutions are valid merely until such time as they become obsolete. 
> 
> The philosopher of science is apt to look somewhat askance at such methods, but the practical scientist knows that they often achieve success when the classical methods bog down in complexities. How to make edible bread, for instance, was not found by the methods of classical science-had we waited for that we still would not have an edible loaf-but by methods analogous to those of operational research: if a variation works, exploit it further; ask not why it works, only if it works. We must be careful, in fact, not to exaggerate the part played by classical science in present-day civilisation and technology. Consider, for instance, how much empirical and purely practical knowledge plays a part in our knowledge of metallurgy, of lubricants, of house-building, of pottery, and so on. 
> 
> What I suggest is that measurement of the quantity of information, even if it can be done only approximately, will tell the investigator where a complex system falls in relation to his limitation. If it is well below the limit, the classic methods may be appropriate; but should it be above the limit, then if his work is to be realistic and successful, he must alter his strategy to one more like that of operational research. 
> 
> My emphasis on the investigator's limitation may seem merely depressing. That is not at all my intention. The law of requisite variety, and Shannon's theorem 10, in setting a limit to what can be done, may mark this era as the law of conservation of energy marked its era a century ago. When the law of conservation of energy was first pronounced, it seemed at first to be merely negative, merely an obstruction; it seemed to say only that certain things, such as getting perpetual motion, could not be done. Nevertheless, the recognition of that limitation was of the greatest value to engineers and physicists, and it has not yet exhausted its usefulness. I suggest that recognition of the limitation implied by the law of requisite variety may, in time, also prove useful, by ensuring that our scientific strategies for the complex system shall be, not slavish and inappropriate copies of the strategies used in physics and chemistry, but new strategies, genuinely adapted to the special peculiarities of the complex system.

I trust reading Ashby's work has set the scene for his thinking on constraints as the 

From: Snowden on Constraints [Constraints - Cynefin.io](https://cynefin.io/wiki/Constraints)

> ## Types of Constraints
> 
> There are many ways to categorize and describe constraints. The [[and Chaos) in Times of Crisis_](https://cynefin.io/wiki/Field_guide_to_managing_complexity_(and_chaos|_Managing Complexity (and Chaos) in Times of Crisis_]]_in_times_of_crisis "Field guide to managing complexity (and chaos) in times of crisis") describes the following types. Note these should not be seen as exclusive but complementary in a number of cases, for example governing constraints in the Clear domain are also likely to highly rigid and have low permeability.
> 
> ### Governing/Enabling Constraints
> 
> Laws, rules, and codes create governing constraints. They give a sense of stability but are sensitive to change. Heuristics and principles, on the other side, provide guidance while allowing for distributed decision-making. Mining the organisation's narratives for examples of heuristics that have evolved over time, based on expertise and experience, is a key audit process. They are then consolidated, codified in memorable form, and associated with teaching stories for rapid distribution. Measurability of compliance and a focus on concrete are key, abstract platitudes don’t work.
> 
> ### Internal/External Constraints
> 
> Insects have exo-skeletons which limit the size to which they can grow but provide a clearly visible structure; mammals have an endo-skeleton which makes them all self-similar but with a wider variety and fewer limitations on growth. Organisation design tends to focus on creating a skeleton, or scaffolding, and ‘points of coherence’ around which unities interact with each other and with the scaffolding itself. This is the case of ritualized meetings, performance evaluations, career assessments, etc. As far as external boundaries think markets, resources, social foundations, and environmental ceilings.
> 
> ### Connecting/Containing Constraints
> 
> Connections, like hashtags in knowledge management and links in networks, provide a flexible and adaptive structure but at the cost of visibility and control. Containers, like categories, spreadsheets cells, and departments, provide clear, reassuring boundary conditions. Changing connections between people and organisational units is less costly than trying to restructure or reorganize departments. As new connections start to provide new ways of dealing with issues, then the constraints can be tightened and eventually formalized into new units and departments.
> 
> ### Rigid/Flexible/Permeable Constraints
> 
> Deadlines are an example of constraints that are usually intended to be rigid. Flexi-time is a malleable way to manage attendance at work. Rigid structures resist until their design conditions are exceeded at which point they break catastrophically. In contrast, flexible structures adapt to stress and conditions of constant change. Rigid and flexible boundaries increase their [resilience](https://cynefin.io/wiki/Resilience "Resilience") with permeability or special conditions that allow for exceptions, but permeability brings the possibility of clogs, i.e. too many people applying for or expecting exceptions.
> 
> ### Dark Constraints
> 
> A reference to dark energy or dark matter: we can see the effect of a constraint but we don’t know the cause. Dark constraints are like the several hidden meanings a term can assume for different people. When we mention a term and we see different reactions, we see dark constraints at work. Narratives are powerful antidotes against dark constraints. We can also get a sense of the risk going forward by modeling how much of the past we can explain by the constraints we are aware of. The more we can’t explain the less we can monitor, the more likely unexpected and potentially catastrophic surprise.

It is worth reading Corrigan's description of patterns and the role of constraints in using the Cynefin classification for behaviours ([Patterns and constraints – Chris Corrigan](https://www.chriscorrigan.com/parkinglot/patterns-and-constraints/)) 

The apogee of this thinking is in the joint paper with Thurlow on "The Substrate-Independence Theory: Advancing Constructor  Theory to Scaffold Substrate Attributes for the Recursive  Interaction between Knowledge and Information"

> The substrate-independence theory utilizes sensemaking techniques to provide cognitively based scaffolds that guide and structure learning. Scaffolds are cognitive abstractions of constraints that relate to information within a system. The substrate-independence theory concentrates on the flow of information as the underlying property of the host system. The substrate-independence theory views social systems as complex adaptive systems capable of repurposing their structure to combat external threats by utilizing constructors and substrates. Constructor theory is used to identify potential construction tasks, the legitimate input and output states that are possible, to map the desired change in the substrate’s attributes

This quote is extracted from the paper summary which is about as far as I got

The construction of this approach goes ...

after deciding 